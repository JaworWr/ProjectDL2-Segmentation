model:
  module_name: models.unet
  class_name: UnetModel
  channels: [64, 128, 128, 256]
  batch_norm: True
  learning_rate: 1.0e-3

trainer:
#  module_name: trainers.cosine_annealing
#  class_name: CosineAnnealingTrainer
  module_name: trainers.reduce_lr_on_plateau
  class_name: ReduceLROnPlateauTrainer
  tensorboard_enabled: True
  log_directory: experiments/large_unet/logs
  model_checkpoint: True
  epochs: 45
#  lr_min: 3.0e-4
#  lr_max: 3.0e-3
#  run_initial: 4
#  run_mult: 2
  reduce_lr_on_plateau:
    monitor: val_loss
    factor: 0.1
    patience: 8
    min_delta: 1.0e-2

data_preprocessing:
  module_name: data_preprocessing.augmentation
  class_name: AugmentationPreprocessing
  image_size: [140, 284]
  background_class_weight: 0.1

data:
  batch_size: 16
  prefetch: True
  prefetch_buffer_size: 10
  shuffle: True
#  shuffle_buffer_size: 100
  workers: 10

devices:
  memory_growth: False
